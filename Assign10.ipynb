{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a101610-522e-4892-9093-9b59af223d48",
   "metadata": {},
   "source": [
    "Assign 10\n",
    "Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document \n",
    "Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0fc2e7c-a4e1-4ba6-9e73-23ed330112a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1626880-0f2e-491d-962b-53b426a922a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document\n",
    "document = \"Text analytics is the process of deriving insights from text data. It involves various techniques such as tokenization, POS tagging, stop words removal, stemming, and lemmatization.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b967a788-a889-43b9-9be0-15c85515419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6007f8aa-a493-4d03-9a74-43bd7956756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SANKET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1339fef2-1717-4817-b1c6-6e87e7b15cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac0d6cae-4ddb-4f32-84f9-3d1aba6f0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SANKET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3fbfc2f-ed07-4be9-9e7f-d0f245e881d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "586880c6-45d6-4fcd-ba87-1f2229b52caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SANKET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data for stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5749afdf-2b4c-4b66-97eb-3d4ae58f6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38959b85-9fbc-4a86-8e23-c5ad5fc08a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0aae728-b9f7-4c47-874e-937b07df2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SANKET\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6fa3a96-44fb-4cfb-bb68-5f49203b2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = []\n",
    "for word, tag in pos_tags:\n",
    "    wn_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    wn_tag = 'N' if wn_tag == 'J' else wn_tag\n",
    "    if wn_tag not in ('N', 'V', 'R', 'J'):\n",
    "        lemmatized_tokens.append(word)\n",
    "    else:\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wn_tag.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71e3587c-e335-4c93-b997-c407b6d8c511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: Text analytics is the process of deriving insights from text data. It involves various techniques such as tokenization, POS tagging, stop words removal, stemming, and lemmatization.\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Original Document:\", document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76924772-95b0-4b65-a758-85c72e0d54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: ['Text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'insights', 'from', 'text', 'data', '.', 'It', 'involves', 'various', 'techniques', 'such', 'as', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenization:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cf424f8-754d-43b8-a60e-e0f0e9c7a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging: [('Text', 'NN'), ('analytics', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('deriving', 'VBG'), ('insights', 'NNS'), ('from', 'IN'), ('text', 'NN'), ('data', 'NNS'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('various', 'JJ'), ('techniques', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"POS Tagging:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25fbd62d-2810-4211-905b-b1278da301c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words Removal: ['Text', 'analytics', 'process', 'deriving', 'insights', 'text', 'data', '.', 'involves', 'various', 'techniques', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stop Words Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc77849d-e6f3-4aea-803b-fb0ad7242fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['text', 'analyt', 'process', 'deriv', 'insight', 'text', 'data', '.', 'involv', 'variou', 'techniqu', 'token', ',', 'po', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea9536c2-41cb-4113-87ff-dda85c2e4572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization: ['Text', 'analytics', 'be', 'the', 'process', 'of', 'derive', 'insight', 'from', 'text', 'data', '.', 'It', 'involves', 'various', 'technique', 'such', 'as', 'tokenization', ',', 'POS', 'tag', ',', 'stop', 'word', 'removal', ',', 'stem', ',', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatization:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cef9ed-8fe9-4290-945a-bf21df453775",
   "metadata": {},
   "source": [
    "Task 2: Term Frequency and Inverse Document Frequency (TF-IDF)\n",
    "TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents. You can use libraries like scikit-learn to calculate TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1163618c-4289-4b01-b5ac-85bcae64aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "734441c6-7d6c-43f2-9107-39d5a602065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of documents (just one document in this case)\n",
    "documents = [document]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d187cfa-319c-4c88-8104-2a70ca23b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33454b88-8aba-473a-94df-def238eaff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c872a425-b75c-4a6a-81f1-047e6bb0814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebac5930-09f6-431c-8368-b0f89233237f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n",
      "lemmatization: 0.1889822365046136\n",
      "and: 0.1889822365046136\n",
      "stemming: 0.1889822365046136\n",
      "removal: 0.1889822365046136\n",
      "words: 0.1889822365046136\n",
      "stop: 0.1889822365046136\n",
      "tagging: 0.1889822365046136\n",
      "pos: 0.1889822365046136\n",
      "tokenization: 0.1889822365046136\n",
      "as: 0.1889822365046136\n",
      "such: 0.1889822365046136\n",
      "techniques: 0.1889822365046136\n",
      "various: 0.1889822365046136\n",
      "involves: 0.1889822365046136\n",
      "it: 0.1889822365046136\n",
      "data: 0.1889822365046136\n",
      "from: 0.1889822365046136\n",
      "insights: 0.1889822365046136\n",
      "deriving: 0.1889822365046136\n",
      "of: 0.1889822365046136\n",
      "process: 0.1889822365046136\n",
      "the: 0.1889822365046136\n",
      "is: 0.1889822365046136\n",
      "analytics: 0.1889822365046136\n",
      "text: 0.3779644730092272\n"
     ]
    }
   ],
   "source": [
    "# Print TF-IDF representation\n",
    "print(\"TF-IDF Representation:\")\n",
    "for col in tfidf_matrix.nonzero()[1]:\n",
    "    print(f\"{feature_names[col]}: {tfidf_matrix[0, col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf97d0b-7b3a-4d8f-be7f-895c4722dcad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
